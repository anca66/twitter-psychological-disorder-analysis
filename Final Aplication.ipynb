{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import ftfy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tokenizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sn\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import savetxt\n",
    "import joblib \n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweets   Labels\n",
      "11818  The least you can do when you find out someone...  Anxiety\n",
      "19159  if tv makes ME forget that people who have bip...  Bipolar\n",
      "28110  - name: alex\\n- email: astitchhx@gmail.com\\n- ...      NaN\n",
      "18877  also iâ€™m aware i need a haircut  my face loo...  Bipolar\n",
      "19223  I've always believed in the power of sharing s...  Bipolar\n"
     ]
    }
   ],
   "source": [
    "## 01. Incarcarea datelor\n",
    "\n",
    "comments_df = '../Dataset/DFTrain_v1.csv'\n",
    "dataset = pd.read_csv(comments_df,  header = 1, names=['Tweets', 'Labels'])\n",
    "dataset = shuffle(dataset,random_state=1)\n",
    "\n",
    "## 02. Tratarea null-urilor\n",
    "dataset[pd.isnull(dataset)]  = 'NaN'\n",
    "print(dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## 03. Functia de curatare a textului din retelele de socializare cu stemming si eliminare Stop Words\n",
    "def cleanComments(corpus):\n",
    "    comm_cleaned = []\n",
    "    for comm in corpus:\n",
    "        comm = str(comm)\n",
    "        comm = ' '.join(\n",
    "            re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(https:\\\\.*)|(www.*)|(http:\\\\.*)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", comm).split())\n",
    "        \n",
    "        comm = ftfy.fix_text(comm)\n",
    "        comm = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", comm).split())\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(comm)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        \n",
    "        comm = ' '.join(filtered_sentence)\n",
    "\n",
    "        comm = PorterStemmer().stem(comm)\n",
    "\n",
    "        comm_cleaned.append(comm)\n",
    "\n",
    "    return comm_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def splitDataSet(dataset):\n",
    "    ## Impartirea setului de date (70 % date de antrenare, 30 % date de testare)\n",
    "    split_train = round(len(dataset) *0.7)\n",
    "    split_validation = split_train + round(len(dataset) *0.3)\n",
    "    \n",
    "    ## Separarea si curatarea comentariilor\n",
    "    x_train = cleanComments([x for x in dataset['Tweets'][:split_train]])\n",
    "    y_train = [y for y in dataset['Labels'][:split_train]]\n",
    "\n",
    "    x_val =cleanComments([x for x in dataset['Tweets'][split_train:split_validation]])\n",
    "    y_val = [y for y in dataset['Labels'][split_train:split_validation]]\n",
    "    \n",
    "    return (x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anca.hiliuta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "## 0.4 Definirea functiei de stabilire a simbolurilor folosite in corpusul de antrenare\n",
    "\n",
    "def get_vocabulary_char_level(X_train_str):\n",
    "    txt = ''\n",
    "    for doc in X_train_str:\n",
    "        for s in doc:\n",
    "            txt += s\n",
    "    return set(txt)\n",
    "\n",
    "## 0.5 Definirea functiei de creare a tokenizer-ului, la nivel de caracter\n",
    "def create_tokenizer_char_level(X_train_str, chars):\n",
    "\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "\n",
    "    tk.fit_on_texts(X_train_str)\n",
    "\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(chars):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "    tk.word_index = char_dict\n",
    "\n",
    "    tk.word_index[tk.oov_token] = len(char_dict.values()) + 1\n",
    "    return tk\n",
    "\n",
    "## 0.6 Definirea functiei de procesare a comentariilor folosind tokenizer-ul creat anterior\n",
    "## si uniformizarea lungimii secventelor la 280 de caractere \n",
    "def preprocess_dataset(dataset, tk):\n",
    "    sequences = tk.texts_to_sequences(dataset)\n",
    "\n",
    "    proc_data = pad_sequences(sequences, maxlen=280, padding='post')\n",
    "    \n",
    "    proc_data = np.array(proc_data)\n",
    "\n",
    "    return proc_data\n",
    "\n",
    "## 0.7 Definirea functiei ce exporta intr-un fisier csv raportul de clasificare al modelelor\n",
    "def classification_report_to_csv(ground_truth, predictions, full_path):\n",
    "\n",
    "    labels = unique_labels(ground_truth, predictions)\n",
    "\n",
    "    precision, recall, f_score, support = precision_recall_fscore_support(ground_truth, predictions, labels=labels, average=None)\n",
    "    \n",
    "    results_pd = pd.DataFrame({\"class\": labels, \"precision\": precision, \"recall\": recall, \"f_score\": f_score, \"support\": support})\n",
    "\n",
    "    results_pd.to_csv(full_path, index=False)\n",
    "    \n",
    " ## 0.8 Definirea  functiei de antrenare al modelului   \n",
    "def model(model,title,X_train,y_train, X_test, y_test):\n",
    "    classifier = model\n",
    "    ## Monitorizarea timpului de executie pentru fiecare model antrenat    \n",
    "    start = time.process_time()\n",
    "    trainedmodel=classifier.fit(X_train, y_train)\n",
    "    print(time.process_time() - start)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    ## Afisarea valorii acuratetei modelului\n",
    "    print('Pentru modelul #', str(title) ,accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    ## Crearea unui fisier csv cu scorurile modelului - pentru partea de raportare   \n",
    "    classification_report_to_csv(y_test, y_pred,str(title)+'_CR'+'.csv')\n",
    "\n",
    "    return trainedmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train,  x_test, y_test) = splitDataSet(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'8': 1, 'p': 2, 'u': 3, 'A': 4, 'y': 5, 'o': 6, 'r': 7, '5': 8, '1': 9, 's': 10, 't': 11, 'z': 12, 'g': 13, 'c': 14, '7': 15, '3': 16, '4': 17, 'x': 18, '9': 19, 'm': 20, '2': 21, 'n': 22, 'w': 23, '0': 24, 'h': 25, 'q': 26, 'f': 27, 'i': 28, 'k': 29, 'l': 30, '6': 31, ' ': 32, 'v': 33, 'a': 34, 'j': 35, 'e': 36, 'b': 37, 'I': 38, 'd': 39, 'UNK': 40}\n",
      "Vocabular Size is  40\n"
     ]
    }
   ],
   "source": [
    "chars = get_vocabulary_char_level(x_train)\n",
    "tk = create_tokenizer_char_level(x_train, chars)  \n",
    "maxlen = 280\n",
    "vocab_size = len(tk.word_index)\n",
    "print(\"vocabulary: \", tk.word_index)\n",
    "print('Vocabular Size is ', vocab_size)\n",
    "# Salvarea tokenizer-ului pentru a fi folosit ulterior in aplicatia web\n",
    "data_dir = '../result'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "with open('../result/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 09. Crearea Setului de date procesat - la nivel de caracter - pentru partea de DL\n",
    "train_data = preprocess_dataset(x_train, tk)\n",
    "test_data = preprocess_dataset(x_test, tk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "c = {'NaN':0, 'Depression':1, 'Bipolar':2, 'PTSD':3, 'Anxiety':4}\n",
    "integer_mapping = {x: i for i,x in enumerate(c)}\n",
    "y_train_one_hot= [integer_mapping[word] for word in y_train]\n",
    "y_test_one_hot = [integer_mapping[word] for word in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## 10. Crearea unei functii de parsare a modelelor corespunzatoare invatarii automate\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(x_train)\n",
    "trainx = vec.transform(x_train)\n",
    "testx = vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=3)]: Done 280 out of 280 | elapsed:   30.4s finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=3)]: Done 280 out of 280 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pentru modelul # Random forest 0.9026382602936306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85      1568\n",
      "           1       0.95      0.89      0.92      2298\n",
      "           2       0.92      0.89      0.90      1544\n",
      "           3       0.93      0.88      0.91      1543\n",
      "           4       0.90      0.94      0.92      2106\n",
      "\n",
      "    accuracy                           0.90      9059\n",
      "   macro avg       0.90      0.90      0.90      9059\n",
      "weighted avg       0.91      0.90      0.90      9059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../result/Random Forest/trainedRandomForest.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 11. RANDOM FOREST \n",
    "trainedRandomForest = model(RandomForestClassifier(n_estimators=280, n_jobs =3,criterion ='gini',class_weight='balanced_subsample', min_samples_split = 18,verbose = 1), 'Random forest', trainx,y_train_one_hot,testx,y_test_one_hot)\n",
    "\n",
    "## 11.1 Salvarea modelului intr-un fisier pickle \n",
    "data_dir = '../result/Random Forest'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "joblib.dump(trainedRandomForest, '../result/Random Forest/trainedRandomForest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.625\n",
      "Pentru modelul # Random forest standard 0.9015343856937852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84      1568\n",
      "           1       0.91      0.90      0.91      2298\n",
      "           2       0.93      0.89      0.91      1544\n",
      "           3       0.94      0.89      0.91      1543\n",
      "           4       0.91      0.94      0.93      2106\n",
      "\n",
      "    accuracy                           0.90      9059\n",
      "   macro avg       0.90      0.90      0.90      9059\n",
      "weighted avg       0.90      0.90      0.90      9059\n",
      "\n",
      "50.75\n",
      "Pentru modelul # Random forest 1 0.9039629098134452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      1568\n",
      "           1       0.93      0.91      0.92      2298\n",
      "           2       0.94      0.88      0.91      1544\n",
      "           3       0.93      0.88      0.91      1543\n",
      "           4       0.90      0.95      0.93      2106\n",
      "\n",
      "    accuracy                           0.90      9059\n",
      "   macro avg       0.90      0.90      0.90      9059\n",
      "weighted avg       0.91      0.90      0.90      9059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 11.2 RANDOM FOREST - antrenare cu diferiti parametri\n",
    "trainedRandomForest_standard = model(RandomForestClassifier(), 'Random forest standard', trainx,y_train_one_hot,testx,y_test_one_hot)\n",
    "trainedRandomForest_1 = model(RandomForestClassifier(n_estimators=280,criterion='gini',min_samples_split=18), 'Random forest 1', trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "0.09375\n",
      "Pentru modelul # Rocchio 0.8577105640799205\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.92      0.77      1568\n",
      "           1       0.92      0.84      0.88      2298\n",
      "           2       0.94      0.85      0.89      1544\n",
      "           3       0.95      0.83      0.88      1543\n",
      "           4       0.88      0.86      0.87      2106\n",
      "\n",
      "    accuracy                           0.86      9059\n",
      "   macro avg       0.87      0.86      0.86      9059\n",
      "weighted avg       0.88      0.86      0.86      9059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../result/Rocchio/trainedRocchio.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 11. ROCCHIO\n",
    "%time\n",
    "data_dir = '../result/Rocchio'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "trainedRocchio = model(NearestCentroid(),'Rocchio',trainx,y_train_one_hot,testx,y_test_one_hot)\n",
    "joblib.dump(trainedRocchio, '../result/Rocchio/trainedRocchio.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "0.03125\n",
      "Pentru modelul # MultiNomial NB 0.7335246715973065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.60      0.73      1568\n",
      "           1       0.65      0.75      0.70      2298\n",
      "           2       0.87      0.68      0.77      1544\n",
      "           3       0.86      0.68      0.76      1543\n",
      "           4       0.63      0.88      0.74      2106\n",
      "\n",
      "    accuracy                           0.73      9059\n",
      "   macro avg       0.79      0.72      0.74      9059\n",
      "weighted avg       0.77      0.73      0.73      9059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../result/Multinomial NB/trainedMultinomialNB.pkl']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 13. NAIVE BAYES MULTINOMIAL\n",
    "%time\n",
    "data_dir = '../result/Multinomial NB'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "trainedMultinomialNB = model(MultinomialNB(alpha=0.6,fit_prior=False),'MultiNomial NB',trainx,y_train_one_hot,testx,y_test_one_hot)\n",
    "joblib.dump(trainedMultinomialNB, '../result/Multinomial NB/trainedMultinomialNB.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "0.046875\n",
      "Pentru modelul # MultiNomial NB standard 0.6698311071862236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.47      0.63      1568\n",
      "           1       0.55      0.80      0.65      2298\n",
      "           2       0.93      0.55      0.69      1544\n",
      "           3       0.91      0.50      0.64      1543\n",
      "           4       0.59      0.89      0.71      2106\n",
      "\n",
      "    accuracy                           0.67      9059\n",
      "   macro avg       0.79      0.64      0.67      9059\n",
      "weighted avg       0.75      0.67      0.67      9059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 13.1 NAIVE BAYES MULTINOMIAL Standard\n",
    "%time\n",
    "trainedMultinomialNB = model(MultinomialNB(),'MultiNomial NB standard',trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "5.78125\n",
      "Pentru modelul # Linear SVC 0.9067225963130588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87      1568\n",
      "           1       0.94      0.91      0.92      2298\n",
      "           2       0.91      0.89      0.90      1544\n",
      "           3       0.91      0.89      0.90      1543\n",
      "           4       0.92      0.93      0.93      2106\n",
      "\n",
      "    accuracy                           0.91      9059\n",
      "   macro avg       0.90      0.90      0.90      9059\n",
      "weighted avg       0.91      0.91      0.91      9059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../result/SVC/trainedSVC.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 14. LINEAR SVC\n",
    "%time\n",
    "data_dir = '../result/SVC'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "trainedSVC = model(LinearSVC(multi_class= 'crammer_singer',max_iter= 10000,class_weight = 'balanced',dual = False), 'Linear SVC', trainx, y_train_one_hot, testx, y_test_one_hot)\n",
    "joblib.dump(trainedSVC, '../result/SVC/trainedSVC.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.96875\n",
      "Pentru modelul # SVC standard 0.9039629098134452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      1568\n",
      "           1       0.94      0.91      0.92      2298\n",
      "           2       0.97      0.86      0.91      1544\n",
      "           3       0.96      0.85      0.90      1543\n",
      "           4       0.92      0.94      0.93      2106\n",
      "\n",
      "    accuracy                           0.90      9059\n",
      "   macro avg       0.91      0.90      0.90      9059\n",
      "weighted avg       0.91      0.90      0.91      9059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 14.1 SVC standard\n",
    "trainedSVC_standard = model(SVC(), 'SVC standard', trainx, y_train_one_hot, testx, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anca.hiliuta\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:20:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "279.828125\n",
      "Pentru modelul # XGB 0.9216249034109725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.86      1568\n",
      "           1       0.97      0.93      0.95      2298\n",
      "           2       0.93      0.89      0.91      1544\n",
      "           3       0.94      0.88      0.91      1543\n",
      "           4       0.96      0.95      0.96      2106\n",
      "\n",
      "    accuracy                           0.92      9059\n",
      "   macro avg       0.92      0.92      0.92      9059\n",
      "weighted avg       0.93      0.92      0.92      9059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../result/XGB/trainedXGB.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 15. XGB\n",
    "%time\n",
    "data_dir = '../result/XGB'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "trainedXGB = model(XGBClassifier(max_depth=10, n_estimators=100, nthread= 3), 'XGB', trainx,y_train_one_hot,testx,y_test_one_hot)\n",
    "joblib.dump(trainedXGB, '../result/XGB/trainedXGB.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "194.125\n",
      "Pentru modelul # XGB standard 0.9211833535710343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.97      0.87      1568\n",
      "           1       0.97      0.92      0.95      2298\n",
      "           2       0.95      0.89      0.92      1544\n",
      "           3       0.94      0.87      0.90      1543\n",
      "           4       0.96      0.95      0.95      2106\n",
      "\n",
      "    accuracy                           0.92      9059\n",
      "   macro avg       0.92      0.92      0.92      9059\n",
      "weighted avg       0.93      0.92      0.92      9059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 15.1 XGB STandard\n",
    "trainedXGB_standard = model(XGBClassifier(), 'XGB standard', trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding weights shape:  (41, 40)\n",
      "embedding weights: \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "## 16. Initializarea matricei de embedding \n",
    "def load_embedding_weights(tk, vocab_size):\n",
    "    embedding_weights = []\n",
    "\n",
    "    embedding_weights.append(np.zeros(vocab_size))\n",
    "\n",
    "    for char, i in tk.word_index.items():\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i - 1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "\n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "\n",
    "    return embedding_weights\n",
    "\n",
    "embedding_size = vocab_size\n",
    "\n",
    "embedding_weights = load_embedding_weights(tk, vocab_size)\n",
    "\n",
    "print(\"embedding weights shape: \", embedding_weights.shape)\n",
    "print(\"embedding weights: \\n\", embedding_weights)\n",
    "\n",
    "input_size = 280\n",
    "num_of_classes = 5\n",
    "dropout_p = 0.2\n",
    "optimizer = 'adam'\n",
    "loss = \"sparse_categorical_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "conv_layers = [[128, 7, 3], [128, 5, -1], [128, 3, -1], [128, 3, 3]]\n",
    "def build_model():\n",
    "    ## 17. Crearea modelului folosind constructorul Sequential\n",
    "    model = Sequential()\n",
    "\n",
    "    ## 17.1 Primul strat al retelei convolutionale: Embedding Layer\n",
    "    model.add(Embedding(vocab_size + 1, embedding_size, input_length=input_size, weights=[embedding_weights]))\n",
    "\n",
    "    ## 17.2 Adaugarea celor 4 straturi convolutionale si a celor 2 straturi de agregare\n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        model.add(Conv1D(filter_num, filter_size, activation='relu'))\n",
    "        if pooling_size != -1:\n",
    "            model.add(MaxPooling1D(pool_size=pooling_size,strides = 3))\n",
    "\n",
    "    ## 17.3 Stratul dintre cel convolutional si cel dens este de tip flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    ## 17.4 Adaugarea stratului de neuroni complet conectati\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    ## 17.5 Pentru a evita supra-ajutarea modelului, am eliminat aleator 20% neuroni\n",
    "    model.add(Dropout(dropout_p))\n",
    "\n",
    "    ## 17.6 Adaugarea ultimului strat de tip dens, ce calculeaza probabilitatea claselor cu ajutorul functiei de activare softmax \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    ## 17.7 Compilarea modelului cu optimizatorul \"adam\", functia de minimizare a erorii de \"sparse_categorical_crossentropy\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    ## 17.8 Afisarea detaliilor retelei create\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 280, 40)           1640      \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 274, 128)          35968     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 91, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 87, 128)           82048     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 85, 128)           49280     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 83, 128)           49280     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 27, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3456)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               884992    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,104,493\n",
      "Trainable params: 1,104,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 18. Antrenarea si evaluarea modelului creat \n",
    "y_test_one_hot= np.array(y_test_one_hot)\n",
    "y_train_one_hot = np.array(y_train_one_hot)\n",
    "\n",
    "model_dp = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236.53125\n"
     ]
    }
   ],
   "source": [
    "## 19. Antrenarea retelei convolutionale si afisarea timpului de antrenare\n",
    "start = time.process_time()\n",
    "\n",
    "history = model_dp.fit(train_data, y_train_one_hot, validation_data=(test_data, y_test_one_hot), batch_size=64, epochs=5, verbose=False)\n",
    "\n",
    "print(time.process_time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelul s-a salvat cu succes\n"
     ]
    }
   ],
   "source": [
    "## 20. Salvarea retelei neuronale convolutionale antrenate intr-un fisier de tip json\n",
    "data_dir = '../result/CNN'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "model_json = model_dp.to_json()\n",
    "with open('../result/CNN/model_CNN.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "## 21. Salvarea ponderilor retelei intr-un fisier de tip HDF5\n",
    "model_dp.save_weights('../result/CNN/model_CNN.h5')\n",
    "print(\"Modelul s-a salvat cu succes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.8934\n"
     ]
    }
   ],
   "source": [
    "## 22. Testarea retelei convolutionale\n",
    "loss, accuracy = model_dp.evaluate(test_data, y_test_one_hot, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "predict_labels = model_dp.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.94      0.83      1568\n",
      "           1       0.93      0.89      0.91      2298\n",
      "           2       0.93      0.85      0.89      1544\n",
      "           3       0.95      0.84      0.89      1543\n",
      "           4       0.93      0.93      0.93      2106\n",
      "\n",
      "    accuracy                           0.89      9059\n",
      "   macro avg       0.90      0.89      0.89      9059\n",
      "weighted avg       0.90      0.89      0.89      9059\n",
      "\n",
      "Predicted     0     1     2     3     4\n",
      "Actual                                 \n",
      "0          1472    50    28     6    12\n",
      "1           108  2044    31    26    89\n",
      "2           162    36  1320     9    17\n",
      "3           175    20    14  1296    38\n",
      "4            60    45    19    21  1961\n",
      "Incarcarea modelului de pe disk\n"
     ]
    }
   ],
   "source": [
    " ## 23. Crearea raportului de sinteza al retelei \n",
    "final_label_prediction=[]\n",
    "\n",
    "for j in range(0,len(predict_labels)):\n",
    "    max= 0\n",
    "    for i in range(0,len(predict_labels[j])):\n",
    "        if predict_labels[j][i] > max:\n",
    "            max = predict_labels[j][i]\n",
    "            i_max= i\n",
    "    final_label_prediction.append(i_max)\n",
    "\n",
    "## 23.1 Crearea si salvarea raportului de clasificare al retelei neuronale\n",
    "print(classification_report(y_test_one_hot, final_label_prediction))\n",
    "classification_report_to_csv(y_test_one_hot, final_label_prediction,'CNN_CR.csv')\n",
    "\n",
    "## 23.2 Crearea si stocarea intr-un fisier csv a matricei de confuzie al CNN\n",
    "y_true = pd.Series(y_test_one_hot, name=\"Actual\")\n",
    "y_pred = pd.Series(final_label_prediction, name=\"Predicted\")\n",
    "df_confusion = pd.crosstab(y_true, y_pred)\n",
    "print (df_confusion)\n",
    "df_confusion.to_csv('../result/CNN/Matriceadeconfuzie.csv')\n",
    "\n",
    "\n",
    "## 24. Incarcarea modelului json si recrearea modelului CNN\n",
    "json_file = open('../result/CNN/model_CNN.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "## 24.1 Incarcarea ponderilor retelei convolutionale create\n",
    "loaded_model.load_weights('../result/CNN/model_CNN.h5')\n",
    "print(\"Incarcarea modelului de pe disk\")\n",
    "model_CNN = model_from_json(loaded_model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## 25. Initializare parametrilor idf si a vocabularului\n",
    "idfs = np.array(vec.idf_)\n",
    "vocabulary = vec.vocabulary_\n",
    "\n",
    "## 25.1 Crearea unei functii de initializare a acestor valori pe TF-IDF vectorize pentru a salva parametrii\n",
    "class MyVectorizer(TfidfVectorizer):\n",
    "    TfidfVectorizer.idf_ = idfs\n",
    "    TfidfVectorizer.vocabulary_ = vocabulary\n",
    "    \n",
    "vectorizer = MyVectorizer(lowercase = False,  min_df = 2,  norm = 'l2',smooth_idf = True)\n",
    "\n",
    "vectorizer._tfidf._idf_diag = sp.spdiags(idfs, diags = 0, m = len(idfs),n = len(idfs))\n",
    "\n",
    "## 25.2 Salvarea in fisere de tip pickle a parametrii vectorizarii\n",
    "with open ('../result/vocabulary_all.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer.vocabulary_,f)\n",
    "with open ('../result/idfs_all.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer.idf_,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done 280 out of 280 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comentariul:  My depression period is fine now, hope for the best :)  este catalogat de \n",
      "\n",
      " Algoritmul Forestier Aleator:  ['Depression']\n",
      " XGB:  ['Depression']\n",
      " SVC Liniar:  ['Depression']\n",
      " ROCCHIO:  ['Depression']\n",
      " Naive Bayes Multinomial:  ['Depression']\n",
      " CNN :  ['Depression']\n"
     ]
    }
   ],
   "source": [
    "## 26. Introducerea comentariului de verificat\n",
    "x_test_nou = [\"My depression period is fine now, hope for the best :)\"]\n",
    "\n",
    "\n",
    "## 27. Testarea modelului CNN \n",
    "test_data_nou = preprocess_dataset(x_test_nou, tk)\n",
    "predict_labels_nou = model_CNN.predict(test_data_nou)\n",
    "\n",
    "prob_max=[]\n",
    "\n",
    "for j in range(0,len(predict_labels_nou)):\n",
    "    max= 0\n",
    "    for i in range(0,len(predict_labels_nou[j])):\n",
    "        if predict_labels_nou[j][i] > max:\n",
    "            max = predict_labels_nou[j][i]\n",
    "            i_max= i\n",
    "    prob_max.append(i_max)\n",
    "    \n",
    "c = {'NaN':0, 'Depression':1, 'Bipolar':2, 'PTSD':3, 'Anxiety':4}\n",
    "integer_mapping = {i: x for i,x in enumerate(c)}\n",
    "final_prediction_CNN = [integer_mapping[word] for word in prob_max]\n",
    "\n",
    "\n",
    "\n",
    "## 28. Initializarea obiectului de tip TF-IDF Vectorizer cu parametrii: vocabulary si idfs\n",
    "vectorizer = TfidfVectorizer(decode_error='ignore')\n",
    "vocabulary = pickle.load(open('../result/vocabulary_all.pkl', mode = 'rb'))\n",
    "idfs = pickle.load(open('../result/idfs_all.pkl', mode = 'rb'))\n",
    "\n",
    "vectorizer.vocabulary_ = vocabulary\n",
    "vectorizer.idf_ = idfs\n",
    "\n",
    "## 29. Transformarea datelor de intrare\n",
    "\n",
    "testx= vec.transform(x_test_nou)\n",
    "\n",
    "## 30. Incarcarea modelelor apriori antrenate\n",
    "model_RF = joblib.load('../result/Random Forest/trainedRandomForest.pkl') \n",
    "model_XGB = joblib.load('../result/XGB/trainedXGB.pkl') \n",
    "model_SVC = joblib.load('../result/SVC/trainedSVC.pkl') \n",
    "model_Rocchio = joblib.load('../result/Rocchio/trainedRocchio.pkl') \n",
    "model_MNB = joblib.load('../result/Multinomial NB/trainedMultinomialNB.pkl') \n",
    "\n",
    "## 31. Afisarea predictiei\n",
    "y_RF = model_RF.predict(testx)\n",
    "y_XGB = model_XGB.predict(testx)\n",
    "y_SVC = model_SVC.predict(testx)\n",
    "y_ROCCHIO = model_Rocchio.predict(testx)\n",
    "y_MNB = model_MNB.predict(testx)\n",
    "\n",
    "## 32. Maparea indicilor numerici cu tulburarea afectiva\n",
    "c = {'NaN':0, 'Depression':1, 'Bipolar':2, 'PTSD':3, 'Anxiety':4}\n",
    "integer_mapping = {i: x for i,x in enumerate(c)}\n",
    "final_prediction_RF = [integer_mapping[word] for word in y_RF]\n",
    "final_prediction_XGB = [integer_mapping[word] for word in y_XGB]\n",
    "final_prediction_SVC = [integer_mapping[word] for word in y_SVC]\n",
    "final_prediction_ROCCHIO = [integer_mapping[word] for word in y_ROCCHIO]\n",
    "final_prediction_MNB = [integer_mapping[word] for word in y_MNB]\n",
    "\n",
    "for comms in x_test_nou:\n",
    "    print('Comentariul: ', comms, ' este catalogat de \\n')\n",
    "    print(' Algoritmul Forestier Aleator: ',final_prediction_RF)\n",
    "    print(' XGB: ',final_prediction_XGB)\n",
    "    print(' SVC Liniar: ',final_prediction_SVC)\n",
    "    print(' ROCCHIO: ',final_prediction_ROCCHIO)\n",
    "    print(' Naive Bayes Multinomial: ',final_prediction_MNB)\n",
    "    print(' CNN : ',final_prediction_CNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
