{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 01. Data loading\n",
    "comments_df = 'Dataset/DataSource_wLabel.csv'\n",
    "dataset = pd.read_csv(comments_df,  header = 1, names=['Tweets', 'Labels'])\n",
    "dataset = shuffle(dataset,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweets   Labels\n",
      "25936                         I hate PTSD and also cops.     PTSD\n",
      "20063  in my life I suffer from bipolar  serious depr...  Bipolar\n",
      "14700  FREE STANDING FIRM WEBINAR  MONDAY  JUNE 1  7-...  Anxiety\n",
      "11830  Yes we are struggling for our rights. Itâ€™s P...  Anxiety\n",
      "17703  Former Utah State football coach Brent Guy ope...  Bipolar\n"
     ]
    }
   ],
   "source": [
    "## 02. Null transforming to NaN\n",
    "dataset[pd.isnull(dataset)]  = 'Neutral'\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Tweets      Labels\n",
      "count                                               30197       30197\n",
      "unique                                              29228           5\n",
      "top     Trump has managed to bring back the:\\n1918 Pan...  Depression\n",
      "freq                                                   57        7563\n"
     ]
    }
   ],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments (twitter + facebook):  30197\n",
      "Number of Categories 5\n",
      "List of Categories ['PTSD' 'Bipolar' 'Anxiety' 'Depression' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "# print out some stats about the data\n",
    "print('Number of comments (twitter + facebook): ', dataset.shape[0])  # .shape[0] gives the rows \n",
    "\n",
    "# .unique() gives unique items in a specified column\n",
    "print('Number of Categories', (len(dataset['Labels'].unique())))\n",
    "print('List of Categories', ( dataset['Labels'].unique()  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxiety</td>\n",
       "      <td>7211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bipolar</td>\n",
       "      <td>5228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Depression</td>\n",
       "      <td>7563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neutral</td>\n",
       "      <td>5068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PTSD</td>\n",
       "      <td>5127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Labels  Counts\n",
       "0     Anxiety    7211\n",
       "1     Bipolar    5228\n",
       "2  Depression    7563\n",
       "3     Neutral    5068\n",
       "4        PTSD    5127"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Distribution\n",
    "# Show counts by Category\n",
    "counts_per_category=dataset.groupby(['Labels']).size().reset_index(name=\"Counts\")\n",
    "display(counts_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAEvCAYAAABolJlEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaRUlEQVR4nO3dfbRl9V3f8fcnQ4JjIgHChdIZ4pA6RiE1JNxS0lgbNZHxYWXQFXSyYplY2lFKjNqmLrC6TNvVllWXD8UIlpVEBiXSMTFhYiVmOk1WfKCZ3CHIMBDKGAhMQWaipkKMExm//WP/CHuGy9wzcJh7f5f3a62z9t7f/XD3/p197+fsh7NvqgpJkrT0PW+xV0CSJE3G0JYkqROGtiRJnTC0JUnqhKEtSVInDG1Jkjpx3GKvwEJOOeWUWrNmzWKvhiRJx8TOnTs/X1Uz841b8qG9Zs0a5ubmFns1JEk6JpJ87qnGeXpckqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkTS/7Z49JzTrLYa7C0VC32GkhLhkfakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdeO49e9znOh/K5zpLUjc80pYkqROGtiRJnVgwtJO8PMlto9dfJvnxJCcn2ZbkntY9aTTPFUn2JLk7yQWj+rlJdrVxVyWeq5YkaVILhnZV3V1V51TVOcC5wF8BHwQuB7ZX1VpgexsmyVnABuBsYB1wdZIVbXHXAJuAte21bqpbI0nSMna0p8e/HfiTqvocsB7Y3OqbgQtb/3rgxqo6UFX3AnuA85KcDpxQVbdUVQHXj+aRJEkLONrQ3gD8Zus/raoeAmjdU1t9FfDAaJ69rbaq9R9elyRJE5g4tJO8AHgj8FsLTTpPrY5Qn+9nbUoyl2Ru//79k66iJEnL2tEcaX8ncGtVPdyGH26nvGndfa2+FzhjNN9q4MFWXz1P/Umq6tqqmq2q2ZmZmaNYRUmSlq+jCe0388SpcYCtwMbWvxG4aVTfkOT4JGcy3HC2o51CfyTJ+e2u8YtH80iSpAVM9ES0JF8NvAH44VH5SmBLkkuA+4GLAKpqd5ItwJ3AY8BlVXWwzXMpcB2wEri5vSRJ0gRSS/wxlrOzszU3Nze9BfrV8EMt8ff/Ocl99FDuo3qOSbKzqmbnG+cT0SRJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdMLQlSeqEoS1JUicMbUmSOmFoS5LUCUNbkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJyYK7SQnJnl/ks8kuSvJa5KcnGRbknta96TR9Fck2ZPk7iQXjOrnJtnVxl2VJM/GRkmStBxNeqT9X4GPVNU3AK8E7gIuB7ZX1VpgexsmyVnABuBsYB1wdZIVbTnXAJuAte21bkrbIUnSsrdgaCc5AfgW4D0AVfXlqvoCsB7Y3CbbDFzY+tcDN1bVgaq6F9gDnJfkdOCEqrqlqgq4fjSPJElawCRH2i8D9gO/luTTSd6d5IXAaVX1EEDrntqmXwU8MJp/b6utav2H158kyaYkc0nm9u/ff1QbJEnScjVJaB8HvBq4pqpeBXyRdir8Kcx3nbqOUH9yseraqpqtqtmZmZkJVlGSpOVvktDeC+ytqk+24fczhPjD7ZQ3rbtvNP0Zo/lXAw+2+up56pIkaQILhnZV/SnwQJKXt9K3A3cCW4GNrbYRuKn1bwU2JDk+yZkMN5ztaKfQH0lyfrtr/OLRPJIkaQHHTTjdjwI3JHkB8FnghxgCf0uSS4D7gYsAqmp3ki0Mwf4YcFlVHWzLuRS4DlgJ3NxekiRpAhlu5F66Zmdna25ubnoL9Kvhh1ri7/9zkvvoodxH9RyTZGdVzc43zieiSZLUCUNbkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdOG6xV0DLQLLYa7B0VC32GkhaxjzSliSpE4a2JEmd8PS4JOnoeEnsUMfwsthER9pJ7kuyK8ltSeZa7eQk25Lc07onjaa/IsmeJHcnuWBUP7ctZ0+SqxLfeUmSJnU0p8e/tarOqarZNnw5sL2q1gLb2zBJzgI2AGcD64Crk6xo81wDbALWtte6Z74JkrSAxNfjL3XtmVzTXg9sbv2bgQtH9Rur6kBV3QvsAc5LcjpwQlXdUlUFXD+aR5IkLWDS0C7go0l2JtnUaqdV1UMArXtqq68CHhjNu7fVVrX+w+uSJGkCk96I9tqqejDJqcC2JJ85wrTznX+pI9SfvIDhg8EmgJe+9KUTrqIkScvbREfaVfVg6+4DPgicBzzcTnnTuvva5HuBM0azrwYebPXV89Tn+3nXVtVsVc3OzMxMvjWSJC1jC4Z2khcm+ZrH+4HvAO4AtgIb22QbgZta/1ZgQ5Ljk5zJcMPZjnYK/ZEk57e7xi8ezSNJkhYwyenx04APtm9nHQe8r6o+kuRTwJYklwD3AxcBVNXuJFuAO4HHgMuq6mBb1qXAdcBK4Ob2kiRJE0gt8Wclz87O1tzc3PQW6FceDjWN9982fYLtOX226XTZntM35RxNsnP09epD+BhTSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdMLQlSeqEoS1JUicMbUmSOmFoS5LUCUNbkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJwxtSZI6MXFoJ1mR5NNJfqcNn5xkW5J7Wvek0bRXJNmT5O4kF4zq5ybZ1cZdlSTT3RxJkpavoznS/jHgrtHw5cD2qloLbG/DJDkL2ACcDawDrk6yos1zDbAJWNte657R2kuS9BwyUWgnWQ18N/DuUXk9sLn1bwYuHNVvrKoDVXUvsAc4L8npwAlVdUtVFXD9aB5JkrSASY+0fwn4SeBvR7XTquohgNY9tdVXAQ+Mptvbaqta/+F1SZI0gQVDO8n3APuqaueEy5zvOnUdoT7fz9yUZC7J3P79+yf8sZIkLW+THGm/FnhjkvuAG4FvS/IbwMPtlDetu69Nvxc4YzT/auDBVl89T/1JquraqpqtqtmZmZmj2BxJkpavBUO7qq6oqtVVtYbhBrP/VVU/CGwFNrbJNgI3tf6twIYkxyc5k+GGsx3tFPojSc5vd41fPJpHkiQt4LhnMO+VwJYklwD3AxcBVNXuJFuAO4HHgMuq6mCb51LgOmAlcHN7SZKkCWS4kXvpmp2drbm5uekt0K+GH2oa779t+gTbc/ps0+myPadvyjmaZGdVzc43zieiSZLUCUNbkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdMLQlSeqEoS1JUicMbUmSOmFoS5LUiQVDO8lXJdmR5I+T7E7y71r95CTbktzTuieN5rkiyZ4kdye5YFQ/N8muNu6qJHl2NkuSpOVnkiPtA8C3VdUrgXOAdUnOBy4HtlfVWmB7GybJWcAG4GxgHXB1khVtWdcAm4C17bVuepsiSdLytmBo1+DRNvj89ipgPbC51TcDF7b+9cCNVXWgqu4F9gDnJTkdOKGqbqmqAq4fzSNJkhYw0TXtJCuS3AbsA7ZV1SeB06rqIYDWPbVNvgp4YDT73lZb1foPr0uSpAlMFNpVdbCqzgFWMxw1v+IIk893nbqOUH/yApJNSeaSzO3fv3+SVZQkadk7qrvHq+oLwMcZrkU/3E5507r72mR7gTNGs60GHmz11fPU5/s511bVbFXNzszMHM0qSpK0bE1y9/hMkhNb/0rg9cBngK3AxjbZRuCm1r8V2JDk+CRnMtxwtqOdQn8kyfntrvGLR/NIkqQFHDfBNKcDm9sd4M8DtlTV7yS5BdiS5BLgfuAigKranWQLcCfwGHBZVR1sy7oUuA5YCdzcXpIkaQIZbuReumZnZ2tubm56C/Sr4Yeaxvtvmz7B9pw+23S6bM/pm3KOJtlZVbPzjfOJaJIkdcLQliSpE4a2JEmdMLQlSeqEoS1JUicMbUmSOmFoS5LUCUNbkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpEwuGdpIzknwsyV1Jdif5sVY/Ocm2JPe07kmjea5IsifJ3UkuGNXPTbKrjbsqSZ6dzZIkafmZ5Ej7MeBfV9U3AucDlyU5C7gc2F5Va4HtbZg2bgNwNrAOuDrJirasa4BNwNr2WjfFbZEkaVlbMLSr6qGqurX1PwLcBawC1gOb22SbgQtb/3rgxqo6UFX3AnuA85KcDpxQVbdUVQHXj+aRJEkLOKpr2knWAK8CPgmcVlUPwRDswKltslXAA6PZ9rbaqtZ/eH2+n7MpyVySuf379x/NKkqStGxNHNpJXgR8APjxqvrLI006T62OUH9yseraqpqtqtmZmZlJV1GSpGVtotBO8nyGwL6hqn67lR9up7xp3X2tvhc4YzT7auDBVl89T12SJE1gkrvHA7wHuKuqfmE0aiuwsfVvBG4a1TckOT7JmQw3nO1op9AfSXJ+W+bFo3kkSdICjptgmtcC/xTYleS2Vvsp4EpgS5JLgPuBiwCqaneSLcCdDHeeX1ZVB9t8lwLXASuBm9tLkiRNIMON3EvX7Oxszc3NTW+BfjX8UNN4/23TJ9ie02ebTpftOX1TztEkO6tqdr5xPhFNkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ0wtCVJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdMLQlSeqEoS1JUicMbUmSOmFoS5LUCUNbkqROLBjaSd6bZF+SO0a1k5NsS3JP6540GndFkj1J7k5ywah+bpJdbdxVSTL9zZEkafma5Ej7OmDdYbXLge1VtRbY3oZJchawATi7zXN1khVtnmuATcDa9jp8mZIk6QgWDO2q+gTw54eV1wObW/9m4MJR/caqOlBV9wJ7gPOSnA6cUFW3VFUB14/mkSRJE3i617RPq6qHAFr31FZfBTwwmm5vq61q/YfXJUnShKZ9I9p816nrCPX5F5JsSjKXZG7//v1TWzlJknr2dEP74XbKm9bd1+p7gTNG060GHmz11fPU51VV11bVbFXNzszMPM1VlCRpeXm6ob0V2Nj6NwI3jeobkhyf5EyGG852tFPojyQ5v901fvFoHkmSNIHjFpogyW8CrwNOSbIX+FngSmBLkkuA+4GLAKpqd5ItwJ3AY8BlVXWwLepShjvRVwI3t5ckSZpQhpu5l67Z2dmam5ub3gL9evihpvH+26ZPsD2nzzadLttz+qaco0l2VtXsfON8IpokSZ0wtCVJ6oShLUlSJwxtSZI6YWhLktQJQ1uSpE4Y2pIkdcLQliSpE4a2JEmdMLQlSeqEoS1JUicMbUmSOmFoS5LUCUNbkqROGNqSJHXC0JYkqROGtiRJnTC0JUnqhKEtSVInDG1JkjphaEuS1AlDW5KkThjakiR1wtCWJKkThrYkSZ045qGdZF2Su5PsSXL5sf75kiT16piGdpIVwK8A3wmcBbw5yVnHch0kSerVsT7SPg/YU1WfraovAzcC64/xOkiS1KVjHdqrgAdGw3tbTZIkLeC4Y/zzMk+tnjRRsgnY1AYfTXL3s7pWi+MU4POLvRJkvrekS7bn9Nmm02V7Tt9ybdOvfaoRxzq09wJnjIZXAw8ePlFVXQtce6xWajEkmauq2cVej+XC9pw+23S6bM/pey626bE+Pf4pYG2SM5O8ANgAbD3G6yBJUpeO6ZF2VT2W5G3A7wErgPdW1e5juQ6SJPXqWJ8ep6p+F/jdY/1zl6Blffp/Edie02ebTpftOX3PuTZN1ZPuA5MkSUuQjzGVJKkThvaUJDmY5LYkdyT5rSSr2vBtSf40yf8dDb8gyb9NsjvJ7a32D9tyPt4e83p7ks8keVeSExd58551o/b74yS3JvlHrf53k7z/GSz30emtZT+SfG+SSvINz2AZv7vQvpfkp57u8peK0b63u+1//yrJkvnbmOSNy+2Rz23f/PnR8DuSvPNpLuvEJP/yac57X5JTns68i2XJ7JjLwJeq6pyqegXwZeAH2vA5wK8CvzgaPhf4HuDVVfVNwOs59KEzb2n1bwIOADcdw+1YLI+33yuBK4D/DFBVD1bVm47FCmSwXH4n3gz8AcM3NJ6WqvquqvrCApN1H9o8se+dDbwB+C7gZ6ex4Pbo5mekqrZW1ZXTWJ8l5ADwfVMKzBOBeUN7Gu2/1CyXP1BLze8DX3eE8acDn6+qAwBV9fmqmu/76l8GfhJ4aZJXPitrujSdAPwFQJI1Se5o/W9NclOSj7SzEV/5w9qOju5orx8/fIFJXpRkezuK35Vk/Wj5dyW5GriVQ58j0KUkLwJeC1xCC+0kr2tncd7fzuDc0D6kvLi15cvbdL+Z5F+0/q8chST5wSQ72hHpf0uyIsmVwMpWuyHJf0jyY6P1+I9J3n6st/+ZqKp9DA92eltrnxVJfi7Jp9rZrx+Gr7TnJ5J8MMmdSX718Q98SR5N8u+TfBJ4zVO03Yok17X9dVeSn2jzvr0t7/YkN7baW5O8q/V/bduPb2/dl7b6dUmuSvJHST6b5Jh80H0GHmO4iewnDh+RZCbJB1qbfyrJa1v9nUneMZrujiRrgCuBv9fa9+fae/OxJO8DdrVpP5RkZ4azKZsO/5ldqSpfU3gBj7bucQxHxpeOxr0TeMdo+EXAbcD/Aa4G/slo3MeB2cOW/SGGI/dF385nsf0Otjb5DPD/gHNbfQ1wR+t/K/AQ8BJgJXAHMMtw5mIX8MLWtruBV83zvpzQ+k8B9jA8oW8N8LfA+YvdBlNsyx8E3tP6/wh4NfC61q6rGT6s3wJ8c5vmDW14A/CR0XLua231jcCHgee3+tXAxeP2Hb1Xt7b+5wF/ArxksdtjgvZ6dJ7aXwCnMQT4T7fa8cAccGZrz78GXsbw9dVtwJvadAV8f+uft+3aPrtt9PNObN0HgeMPq70VeFfr/zCwsfX/M+BDrf864Ldau5/F8D8eFr1tj9TmDB/O7wNeDLwDeGcb977RvvlS4K7W/04O/Tt6R9vn1tD+RrT664AvAmeOaie37uN/N14y3scXuz2O5nXMv/K1jK1Mclvr/33gPU81YVU9muRc4B8D3wr89ySXV9V1TzHLsnru4FP4Ug2XDkjyGuD6JK+YZ7ptVfVnbbrfBr6Z4Y/kB6vqi6P6PwY+PZovwH9K8i0MIb2K4Y8ywOeq6n9Pf5MWzZuBX2r9N7bh/wHsqKq9AG1fXQP8QVVtS3IRw3/gm++MzrczhMynMjyucSWw7/CJquq+JH+W5FUMbfvpx9+rDj3+O/cdwDeNjlxfDKxluAS2o6o+C8MZCoZ98f0MH0A/0KZ/qrb7MPCyJL/M8N58tE1/O3BDkg8xfFg/3GuA72v9vw78l9G4D1XV3wJ3JjntSXMuMVX1l0muB94OfGk06vXAWXni0aAnJPmao1z8jqq6dzT89iTf2/rPYHgPu9w3De3p+UroTKKqDjIcVX88yS5gI8On5UNkuCbz94G7prKWHaiqW9pp2Zn5Rs8zPMmHmre05Z1bVX+T5D7gq9q4Lz7ddV1qkrwE+DbgFUmK4SiwGJ6NcGA06UHa7387rfuNDH84T2Z43PAhiwU2V9UVE6zCuxmODP8O8N6nvSGLKMnLGNpnH8O2/2hV/d5h07yO+fdFgL9uv99whLbLcMnrAuAy4PsZjpy/G/gW4I3AzyQ5e4HVHa/D+P3t5YP+LzFclvq1Ue15wGuqahzkJHmMQy/pfhVP7Su/0+29en1b5l8l+fgC8y5pXtNeBElenmTtqHQO8Ll5pns+ww1ZD1TV7cdo9RZdhjueVzD/J+E3JDk5yUrgQuAPgU8AFyb56iQvBL6X4WzH2IuBfS2wv5UjPJC/c28Crq+qr62qNVV1BnAvw1HgU/kJhg+Fbwbe2/a7se3Am5KcCtDa//H2+5vDpv8gsA74BwxPPuxKkhmGG0ffVcP5098DLn18G5N8fdvHAM7L8Ejm5wE/wHDj3+Hmbbv2ofR5VfUB4GeAV7flnFFVH2O4l+VEhss9Y3/EEzcXvuUpfmY3qurPgS0M91887qPA2x4fSHJO672P4VIPSV7NcJkC4BHgSEfiLwb+ogX2NwDnT2PdF4tH2ovjRcAvZ/g6zWMM11fHN0fckOQAwzW0/8lz43+Ojy8vhOG63cE8+b/n/AHDacGvA95XVXMw3IgD7GjTvLuqPn3YfDcAH04yxxPXzpejNzPcmDP2AeBShmvMh0jy9cA/B86rqkeSfAL4aUZ3T1fVnUl+GvhoC5a/YTg6/BzDzUS3J7m1qt5SVV9O8jHgC6OjzaXu8X3v+Qy/j78O/EIb927atfoMO+N+hg+LMNwHcCXDmbBPMHxgOcQR2u5LwK/liW8rXMHwQfU3kryY4XfgF6vqC4f9Dryd4YPVv2nr8kPPdOOXgJ9nFNIM2/grSW5nyKhPAD/CsB9f3N6rTzHcE0RV/VmSP8xww+rNDJcbxj4C/Ehb3t1A15fCfCKaupHkrQw36b1toWm1OFoI3QpcVFX3LPb6PFvaKdd3VNX3LPKq6DnG0+OSpiLJWQxnjbYv58CWFpNH2pIkdcIjbUmSOmFoS5LUCUNbkqROGNqSJHXC0JYkqROGtiRJnfj/WWMcc8wLbmgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "group = ['Labels']\n",
    "counts = dataset.groupby(group).size().reset_index(name=\"Counts\")\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(dataset['Labels'].unique(), counts['Counts'], color = 'red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 03. Functia de curatare a textului din retelele de socializare cu stemming si eliminare Stop Words\n",
    "import ftfy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "def cleanComments(corpus):\n",
    "    comm_cleaned = []\n",
    "    for comm in corpus:\n",
    "        comm = str(comm)\n",
    "        comm = ' '.join(\n",
    "            re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(https:\\\\.*)|(www.*)|(http:\\\\.*)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", comm).split())\n",
    "        \n",
    "        comm = ftfy.fix_text(comm)\n",
    "        comm = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", comm).split())\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = nltk.word_tokenize(comm)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        \n",
    "        comm = ' '.join(filtered_sentence)\n",
    "\n",
    "        comm = PorterStemmer().stem(comm)\n",
    "\n",
    "        comm_cleaned.append(comm)\n",
    "\n",
    "    return comm_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataset):\n",
    "    ## Impartirea setului de date (70 % date de antrenare, 30 % date de testare)\n",
    "    split_train = round(len(dataset) *0.7)\n",
    "    split_validation = split_train + round(len(dataset) *0.3)\n",
    "    \n",
    "    ## Separarea si curatarea comentariilor\n",
    "    x_train = cleanComments([x for x in dataset['Tweets'][:split_train]])\n",
    "    y_train = [y for y in dataset['Labels'][:split_train]]\n",
    "\n",
    "    x_val =cleanComments([x for x in dataset['Tweets'][split_train:split_validation]])\n",
    "    y_val = [y for y in dataset['Labels'][split_train:split_validation]]\n",
    "    \n",
    "    return (x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train,  x_test, y_test) = splitDataSet(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {'Neutral':0, 'Depression':1, 'Bipolar':2, 'PTSD':3, 'Anxiety':4}\n",
    "integer_mapping = {x: i for i,x in enumerate(c)}\n",
    "y_train_one_hot= [integer_mapping[word] for word in y_train]\n",
    "y_test_one_hot = [integer_mapping[word] for word in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 21138, n_features: 31640\n",
      "n_samples: 9059, n_features: 31640\n"
     ]
    }
   ],
   "source": [
    "## 10. Crearea unei functii de parsare a modelelor corespunzatoare invatarii automate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "trainx = tfidf_vectorizer.fit_transform(x_train)\n",
    "testx = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % trainx.shape)\n",
    "print(\"n_samples: %d, n_features: %d\" % testx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         00  000  000k  000th  00748  00am  00pm   01  01000  0114  ...  zyada  \\\n",
      "0      0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "1      0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "2      0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "3      0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "4      0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "...    ...  ...   ...    ...    ...   ...   ...  ...    ...   ...  ...    ...   \n",
      "21133  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "21134  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "21135  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "21136  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "21137  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "\n",
      "       zydeco  zyklon  zylofon  zyprexa   zz  zzedibles  zznqzvypzfyjuu00  \\\n",
      "0         0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "1         0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "2         0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "3         0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "4         0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "...       ...     ...      ...      ...  ...        ...               ...   \n",
      "21133     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "21134     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "21135     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "21136     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "21137     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "\n",
      "       zzuduiyngoybca00  zzzzzzzzzzzzz  \n",
      "0                   0.0            0.0  \n",
      "1                   0.0            0.0  \n",
      "2                   0.0            0.0  \n",
      "3                   0.0            0.0  \n",
      "4                   0.0            0.0  \n",
      "...                 ...            ...  \n",
      "21133               0.0            0.0  \n",
      "21134               0.0            0.0  \n",
      "21135               0.0            0.0  \n",
      "21136               0.0            0.0  \n",
      "21137               0.0            0.0  \n",
      "\n",
      "[21138 rows x 31640 columns]>\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "dense = trainx.todense()\n",
    "denselist = dense[:]\n",
    "df_train = pd.DataFrame(denselist, columns=feature_names)\n",
    "print(df_train.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of        00  000  000k  000th  00748  00am  00pm   01  01000  0114  ...  zyada  \\\n",
      "0     0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "1     0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "2     0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "3     0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "4     0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "...   ...  ...   ...    ...    ...   ...   ...  ...    ...   ...  ...    ...   \n",
      "9054  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "9055  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "9056  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "9057  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "9058  0.0  0.0   0.0    0.0    0.0   0.0   0.0  0.0    0.0   0.0  ...    0.0   \n",
      "\n",
      "      zydeco  zyklon  zylofon  zyprexa   zz  zzedibles  zznqzvypzfyjuu00  \\\n",
      "0        0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "1        0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "2        0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "3        0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "4        0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "...      ...     ...      ...      ...  ...        ...               ...   \n",
      "9054     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "9055     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "9056     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "9057     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "9058     0.0     0.0      0.0      0.0  0.0        0.0               0.0   \n",
      "\n",
      "      zzuduiyngoybca00  zzzzzzzzzzzzz  \n",
      "0                  0.0            0.0  \n",
      "1                  0.0            0.0  \n",
      "2                  0.0            0.0  \n",
      "3                  0.0            0.0  \n",
      "4                  0.0            0.0  \n",
      "...                ...            ...  \n",
      "9054               0.0            0.0  \n",
      "9055               0.0            0.0  \n",
      "9056               0.0            0.0  \n",
      "9057               0.0            0.0  \n",
      "9058               0.0            0.0  \n",
      "\n",
      "[9059 rows x 31640 columns]>\n"
     ]
    }
   ],
   "source": [
    "dense = testx.todense()\n",
    "denselist = dense[:]\n",
    "df_test = pd.DataFrame(denselist, columns=feature_names)\n",
    "print(df_test.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21138, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD  \n",
    "\n",
    "truncatedSVD = TruncatedSVD(300)\n",
    "X_train_truncated = truncatedSVD.fit_transform(trainx)\n",
    "X_test_truncated = truncatedSVD.transform(testx)\n",
    " \n",
    "print(X_train_truncated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train_truncated)\n",
    "x_test_scaled = scaler.transform(X_test_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    # make data dir, if it does not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    \n",
    "    # your code here\n",
    "    pd.concat([pd.DataFrame(y), pd.DataFrame(x)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, filename), header=True, index=False)\n",
    "    \n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "df_train_all = pd.concat([pd.DataFrame(y_train_one_hot), pd.DataFrame(df_train)], axis=1)\n",
    "with open('Dataset/Train_Dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_train_all, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_all = pd.concat([pd.DataFrame(y_test_one_hot), pd.DataFrame(df_test)], axis=1)\n",
    "with open('Dataset/Test_Dataset.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_test_all, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: Dataset/Scaled_Train_Dataset.csv\n",
      "Path created: Dataset/Scaled_Test_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "make_csv(x_train_scaled, y_train_one_hot, filename='Scaled_Train_Dataset.csv', data_dir='Dataset')\n",
    "make_csv(x_test_scaled, y_test_one_hot, filename='Scaled_Test_Dataset.csv', data_dir='Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21138, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.shape\n",
    "# x_train_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9059, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "## 0.7 Definirea functiei ce exporta intr-un fisier csv raportul de clasificare al modelelor\n",
    "def classification_report_to_csv(ground_truth, predictions, full_path):\n",
    "\n",
    "    labels = unique_labels(ground_truth, predictions)\n",
    "\n",
    "    precision, recall, f_score, support = precision_recall_fscore_support(ground_truth, predictions, labels=labels, average=None)\n",
    "    \n",
    "    results_pd = pd.DataFrame({\"class\": labels, \"precision\": precision, \"recall\": recall, \"f_score\": f_score, \"support\": support})\n",
    "\n",
    "    results_pd.to_csv(full_path, index=False)\n",
    "    \n",
    " ## 0.8 Definirea  functiei de antrenare al modelului   \n",
    "def model(model,title,X_train,y_train, X_test, y_test):\n",
    "    classifier = model\n",
    "    ## Monitorizarea timpului de executie pentru fiecare model antrenat    \n",
    "    start = time.process_time()\n",
    "    trainedmodel=classifier.fit(X_train, y_train)\n",
    "    print(time.process_time() - start)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    ## Afisarea valorii acuratetei modelului\n",
    "    print('Pentru modelul #', str(title) ,accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    ## Crearea unui fisier csv cu scorurile modelului - pentru partea de raportare   \n",
    "    classification_report_to_csv(y_test, y_pred,str(title)+'_CR'+'.csv')\n",
    "\n",
    "    return trainedmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13.1 NAIVE BAYES MULTINOMIAL Standard\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "#trainedMultinomialNB = model(MultinomialNB(),'MultiNomial NB standard',X_truncated,y_train_one_hot,X_test_truncated,y_test_one_hot)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "## 11.2 RANDOM FOREST - antrenare cu diferiti parametri\n",
    "trainedRandomForest_standard = model(RandomForestClassifier(), 'Random forest standard', trainx,y_train_one_hot,testx,y_test_one_hot)\n",
    "# trainedRandomForest_1 = model(RandomForestClassifier(n_estimators=280,criterion='gini',min_samples_split=18), 'Random forest 1', trainx,y_train_one_hot,testx,y_test_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "trainedXGB = model(XGBClassifier(max_depth=10, n_estimators=300, nthread= 3), 'XGB', trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "trainedRocchio = model(NearestCentroid(),'Rocchio',trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainedMultinomialNB = model(MultinomialNB(alpha=0.5,fit_prior=False),'MultiNomial NB standard',trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "trainedSVC = model(LinearSVC(multi_class= 'crammer_singer',max_iter= 10000,class_weight = 'balanced',dual = False), 'Linear SVC', X_truncated, y_train_one_hot, X_test_truncated, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "trainedSVC = model(LinearSVC(multi_class= 'ovr', max_iter= 10000,class_weight = 'balanced'), 'Linear SVC', trainx, y_train_one_hot, testx, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "trainedSVC = model(LinearSVC(multi_class= 'ovr', max_iter= 10000,class_weight = 'balanced'), 'Linear SVC', x_train_scaled, y_train_one_hot, x_test_scaled, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedXGB_standard = model(XGBClassifier(), 'XGB standard', trainx,y_train_one_hot,testx,y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_KNN = model(KNeighborsClassifier(), 'KNN', x_train_scaled,y_train_one_hot,x_test_scaled,y_test_one_hot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ExtraTrees = model(ExtraTreesClassifier(n_estimators=100), 'Extra Trees', x_train_scaled,y_train_one_hot,x_test_scaled,y_test_one_hot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_KNN = model(KNeighborsClassifier(n_neighbors=5), 'KNN', trainx,y_train_one_hot,testx,y_test_one_hot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_RF = model(RandomForestClassifier(n_estimators=300), 'RF', trainx,y_train_one_hot,testx,y_test_one_hot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "trained_ExtraTrees = model(ExtraTreesClassifier(n_estimators=5), 'Extra Trees', trainx,y_train_one_hot,testx,y_test_one_hot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "trained_DT = model(DecisionTreeClassifier(), 'Decision Tree', trainx,y_train_one_hot,testx,y_test_one_hot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_DT = model(DecisionTreeClassifier(), 'Decision Tree', x_train_scaled,y_train_one_hot,x_test_scaled,y_test_one_hot)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "conv_layers = [[128, 7, 3], [128, 5, -1], [128, 3, -1], [128, 3, 3]]\n",
    "\n",
    "embedding_weights = X_truncated\n",
    "vocab_size = X_truncated.shape[0]\n",
    "embedding_size = X_truncated.shape[1]\n",
    "print(\"embedding weights shape: \", embedding_weights.shape)\n",
    "# print(\"embedding weights: \\n\", embedding_weights)\n",
    "\n",
    "input_size = embedding_size\n",
    "num_of_classes = 5\n",
    "dropout_p = 0.2\n",
    "optimizer = 'adam'\n",
    "loss = \"sparse_categorical_crossentropy\"\n",
    "\n",
    "def build_model():\n",
    "    ## 17. Crearea modelului folosind constructorul Sequential\n",
    "    model = Sequential()\n",
    "\n",
    "    ## 17.1 Primul strat al retelei convolutionale: Embedding Layer\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=input_size, weights=[embedding_weights]))\n",
    "\n",
    "    ## 17.2 Adaugarea celor 4 straturi convolutionale si a celor 2 straturi de agregare\n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        model.add(Conv1D(filter_num, filter_size, activation='relu'))\n",
    "        if pooling_size != -1:\n",
    "            model.add(MaxPooling1D(pool_size=pooling_size,strides = 3))\n",
    "\n",
    "    ## 17.3 Stratul dintre cel convolutional si cel dens este de tip flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    ## 17.4 Adaugarea stratului de neuroni complet conectati\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "\n",
    "    ## 17.5 Pentru a evita supra-ajutarea modelului, am eliminat aleator 20% neuroni\n",
    "    model.add(Dropout(dropout_p))\n",
    "\n",
    "    ## 17.6 Adaugarea ultimului strat de tip dens, ce calculeaza probabilitatea claselor cu ajutorul functiei de activare softmax \n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    ## 17.7 Compilarea modelului cu optimizatorul \"adam\", functia de minimizare a erorii de \"sparse_categorical_crossentropy\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "    ## 17.8 Afisarea detaliilor retelei create\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 18. Antrenarea si evaluarea modelului creat \n",
    "y_test_one_hot= np.array(y_test_one_hot)\n",
    "y_train_one_hot = np.array(y_train_one_hot)\n",
    "\n",
    "model_dp = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 19. Antrenarea retelei convolutionale si afisarea timpului de antrenare\n",
    "start = time.process_time()\n",
    "\n",
    "history = model_dp.fit(X_truncated, y_train_one_hot, validation_data=(X_test_truncated, y_test_one_hot), batch_size=64, epochs=5, verbose=False)\n",
    "\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "start = time.process_time()\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(x_train_scaled, y_train_one_hot)\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_test_scaled, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "clf_truncated = MLPClassifier(random_state=1, max_iter=300).fit(X_truncated, y_train_one_hot)\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_truncated.score(X_test_truncated, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "clf_vectorized = MLPClassifier(random_state=1, max_iter=300).fit(trainx, y_train_one_hot)\n",
    "print(time.process_time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_vectorized.score(testx, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 20. Salvarea retelei neuronale convolutionale antrenate intr-un fisier de tip json\n",
    "from tensorflow.keras.models import model_from_json\n",
    "data_dir = 'result/CNN'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "model_json = model_dp.to_json()\n",
    "with open('result/CNN/model_CNN.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "## 21. Salvarea ponderilor retelei intr-un fisier de tip HDF5\n",
    "model_dp.save_weights('result/CNN/model_CNN.h5')\n",
    "print(\"Modelul s-a salvat cu succes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 22. Testarea retelei convolutionale\n",
    "loss, accuracy = model_dp.evaluate(X_test_truncated, y_test_one_hot, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "predict_labels = model_dp.predict(X_test_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_truncated)\n",
    "x_test_scaled = scaler.transform(X_test_truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_truncated[0])\n",
    "print(len(x_train_scaled[0]))\n",
    "# print(y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## 23. Crearea raportului de sinteza al retelei \n",
    "final_label_prediction=[]\n",
    "\n",
    "for j in range(0,len(predict_labels)):\n",
    "    max= 0\n",
    "    for i in range(0,len(predict_labels[j])):\n",
    "        if predict_labels[j][i] > max:\n",
    "            max = predict_labels[j][i]\n",
    "            i_max= i\n",
    "    final_label_prediction.append(i_max)\n",
    "\n",
    "## 23.1 Crearea si salvarea raportului de clasificare al retelei neuronale\n",
    "print(classification_report(y_test_one_hot, final_label_prediction))\n",
    "# classification_report_to_csv(y_test_one_hot, final_label_prediction,'CNN_CR.csv')\n",
    "\n",
    "## 23.2 Crearea si stocarea intr-un fisier csv a matricei de confuzie al CNN\n",
    "y_true = pd.Series(y_test_one_hot, name=\"Actual\")\n",
    "y_pred = pd.Series(final_label_prediction, name=\"Predicted\")\n",
    "df_confusion = pd.crosstab(y_true, y_pred)\n",
    "print (df_confusion)\n",
    "# df_confusion.to_csv('../result/CNN/Matriceadeconfuzie.csv')\n",
    "\n",
    "\n",
    "# ## 24. Incarcarea modelului json si recrearea modelului CNN\n",
    "# json_file = open('../result/CNN/model_CNN.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# ## 24.1 Incarcarea ponderilor retelei convolutionale create\n",
    "# loaded_model.load_weights('../result/CNN/model_CNN.h5')\n",
    "# print(\"Incarcarea modelului de pe disk\")\n",
    "# model_CNN = model_from_json(loaded_model_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
